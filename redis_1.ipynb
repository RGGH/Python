{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edff1774",
   "metadata": {},
   "source": [
    "# Scrapy - Redis Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bcbb9d",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d441d125",
   "metadata": {},
   "source": [
    "### Redis is an open source (BSD licensed), in-memory data structure store, used as a database, cache, and message broker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e1e7a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://redis.io/ \n",
    "# https://try.redis.io/\n",
    "\n",
    "# \"An operation during which a processor can simultaneously \n",
    "# read a location and write it in the same bus operation.\n",
    "# This prevents any other processor or I/O device from writing \n",
    "# or reading memory until the operation is complete\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "936f5b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import redis\n",
    "import json\n",
    "\n",
    "# For Redis set up notes see : https://www.ubuntupit.com/how-to-install-and-configure-redis-on-linux-system/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "046e3848",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RC = Client(host=os.environ.get('REDIS_HOST'),password='M0F0_741!', charset='utf-8', decode_responses=True)\n",
    "\n",
    "RC = redis.Redis(host=os.environ.get('REDIS_HOST'), charset='utf-8', decode_responses=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "724431f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Redis<ConnectionPool<Connection<host=None,port=6379,db=0>>>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RC # Connection object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69ab1394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RC.set(\"foo\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3064331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RC.get(\"foo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8908353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RC.mset({\"Croatia\": \"Zagreb\", \"Bahamas\": \"Nassau\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db6d4ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nassau'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RC.get(\"Bahamas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "747d1cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydict = { 'var1' : 5, 'var2' : 9, 'var3': [1, 5, 9] }\n",
    "rval = json.dumps(mydict)\n",
    "RC.set('key_1', rval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c97cf76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"var1\": 5, \"var2\": 9, \"var3\": [1, 5, 9]}\n"
     ]
    }
   ],
   "source": [
    "data = RC.get('key_1')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23446882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "dc = json.loads(data)\n",
    "print(type(dc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "afc06d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "9\n",
      "[1, 5, 9]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "value too long - so we can't make df with this",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-b9b1f98b0fce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"value too long - so we can't make df with this\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: value too long - so we can't make df with this"
     ]
    }
   ],
   "source": [
    "# check if we can use dict as dataframe as-is\n",
    "for k,v in dc.items():\n",
    "    print(v)\n",
    "    assert len(str(v))==1, \"value too long - so we can't make df with this\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "813d6049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pypi.org/project/scrapy-redis/\n",
    "# You can start multiple spider instances that share a single redis queue. \n",
    "# Best suitable for broad multi-domain crawls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2f5bf690",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scrapy-redis\n",
    "from scrapy_redis.spiders import RedisSpider\n",
    "\n",
    "class MySpider(RedisSpider):\n",
    "    name = 'myspider'\n",
    "\n",
    "    def parse(self, response):\n",
    "        # do stuff\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dc5247",
   "metadata": {},
   "source": [
    "## Feeding a Spider from Redis\n",
    "\n",
    "##### The class scrapy_redis.spiders.RedisSpider enables a spider to read the urls from redis. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "758c79ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The urls in the redis queue will be processed one after another, \n",
    "# if the first request yields more requests, \n",
    "# the spider will process those requests before fetching another url from redis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "35e0d86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pypi.org/project/scrapy-redis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9af55050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(integer) 3\r\n"
     ]
    }
   ],
   "source": [
    "!echo -e 'lpush myspider:start_urls http://google.com https://msn.com http://books.toscrape.com' | redis-cli "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40daa85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-09 15:57:34 [scrapy.utils.log] INFO: Scrapy 2.5.0 started (bot: scrapybot)\n",
      "2021-06-09 15:57:34 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 21.2.0, Python 3.8.8 (default, Apr 13 2021, 19:58:26) - [GCC 7.3.0], pyOpenSSL 20.0.1 (OpenSSL 1.1.1k  25 Mar 2021), cryptography 3.4.7, Platform Linux-5.4.0-73-generic-x86_64-with-glibc2.10\n",
      "2021-06-09 15:57:34 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
      "2021-06-09 15:57:34 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'SPIDER_LOADER_WARN_ONLY': True}\n",
      "2021-06-09 15:57:34 [scrapy.extensions.telnet] INFO: Telnet Password: 2bb08a8270fd9a93\n",
      "2021-06-09 15:57:34 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2021-06-09 15:57:34 [myspider] INFO: Reading start URLs from redis key 'myspider:start_urls' (batch size: 16, encoding: utf-8\n",
      "2021-06-09 15:57:34 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2021-06-09 15:57:34 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2021-06-09 15:57:34 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2021-06-09 15:57:34 [scrapy.core.engine] INFO: Spider opened\n",
      "2021-06-09 15:57:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2021-06-09 15:57:34 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2021-06-09 15:57:34 [py.warnings] WARNING: /home/rag/.local/lib/python3.8/site-packages/scrapy/spiders/__init__.py:81: UserWarning: Spider.make_requests_from_url method is deprecated: it will be removed and not be called by the default Spider.start_requests method in future Scrapy releases. Please override Spider.start_requests method instead.\n",
      "  warnings.warn(\n",
      "\n",
      "2021-06-09 15:57:34 [myspider] DEBUG: Read 3 requests from 'myspider:start_urls'\n",
      "2021-06-09 15:57:34 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.google.com/> from <GET http://google.com>\n",
      "2021-06-09 15:57:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.google.com/> (referer: None)\n",
      "2021-06-09 15:57:35 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.msn.com/> from <GET https://msn.com>\n",
      "2021-06-09 15:57:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://books.toscrape.com> (referer: None)\n",
      "2021-06-09 15:57:35 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.msn.com/en-gb/> from <GET https://www.msn.com/>\n",
      "2021-06-09 15:57:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.msn.com/en-gb/> (referer: None)\n",
      "2021-06-09 15:58:34 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 3 pages/min), scraped 0 items (at 0 items/min)\n",
      "2021-06-09 15:59:34 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2021-06-09 16:00:34 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2021-06-09 16:01:34 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2021-06-09 16:02:34 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2021-06-09 16:03:34 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2021-06-09 16:04:34 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2021-06-09 16:05:34 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2021-06-09 16:06:34 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2021-06-09 16:07:34 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2021-06-09 16:08:34 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2021-06-09 16:09:34 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2021-06-09 16:10:34 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2021-06-09 16:11:34 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2021-06-09 16:12:34 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2021-06-09 16:13:34 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n"
     ]
    }
   ],
   "source": [
    "!scrapy runspider myspider.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a01c3a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
